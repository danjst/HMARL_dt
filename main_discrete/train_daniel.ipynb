{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "#!gpustat --watch 1 --show-user -p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "os.environ['TF_CUDNN_DETERMINISTIC']='2'\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "\n",
    "# sys.path.append('../')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import hierarchy_discrete_alg as alg_discrete\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import argparse\n",
    "import setting\n",
    "from hierarchy_discrete_evaluation_graphs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_col = setting.state_col\n",
    "l_action = setting.number_action\n",
    "str_level = setting.str_level\n",
    "next_state_col = setting.next_state_col\n",
    "action_dis_col = setting.action_dis_col\n",
    "action_con_col = setting.action_con_col\n",
    "ai_action_con_col = setting.ai_action_con_col\n",
    "ai_action_dis_col = setting.ai_action_dis_col\n",
    "ITERATION_ROUND_PRETRAIN = setting.ITERATION_ROUND_PRETRAIN\n",
    "ITERATION_ROUND_QMIX = setting.ITERATION_ROUND_QMIX\n",
    "ITERATION_ROUND_IV = setting.ITERATION_ROUND_IV\n",
    "ITERATION_ROUND_Vaso = setting.ITERATION_ROUND_Vaso\n",
    "ITERATION_ROUND = setting.ITERATION_ROUND\n",
    "TOTAL_ITERATION_NUM = setting.TOTAL_ITERATION_NUM\n",
    "ACTION_SPACE = setting.ACTION_SPACE\n",
    "BATCH_SIZE = setting.BATCH_SIZE\n",
    "STATE_DIM = len(state_col) #48 \n",
    "REWARD_FUN = setting.REWARD_FUN\n",
    "context_state_col = setting.context_state_col\n",
    "context_next_state_col = setting.context_next_state_col\n",
    "hidden_factor = setting.hidden_factor\n",
    "l_state = len(setting.state_col)\n",
    "nn = setting.nn\n",
    "Q_threshold = setting.Q_threshold\n",
    "FM_list = ['FM_' + str(i) for i in range(setting.hidden_factor)]\n",
    "next_FM_list = ['next_FM_' + str(i) for i in range(setting.hidden_factor)]\n",
    "FM_context_list = ['FM_' + str(i) for i in range(setting.hidden_factor)]\n",
    "FM_context_list.extend(['FM_context' + str(i) for i in range(setting.hidden_factor)])\n",
    "next_FM_context_list = ['next_FM_' + str(i) for i in range(setting.hidden_factor)]\n",
    "next_FM_context_list.extend(['next_FM_context' + str(i) for i in range(setting.hidden_factor)])\n",
    "\n",
    "\n",
    "\n",
    "intervals = (\n",
    "    ('weeks', 604800),  # 60 * 60 * 24 * 7\n",
    "    ('days', 86400),    # 60 * 60 * 24\n",
    "    ('hours', 3600),    # 60 * 60\n",
    "    ('minutes', 60),\n",
    "    ('seconds', 1),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_time(seconds, granularity=2):\n",
    "    result = []\n",
    "\n",
    "    for name, count in intervals:\n",
    "        value = seconds // count\n",
    "        if value:\n",
    "            seconds -= value * count\n",
    "            if value == 1:\n",
    "                name = name.rstrip('s')\n",
    "            result.append(\"{} {}\".format(value, name))\n",
    "    return ', '.join(result[:granularity])\n",
    "\n",
    "def compute_master_action(iv_fluids_quantile, vasopressors_quantile):\n",
    "    if iv_fluids_quantile==1 and vasopressors_quantile==1:\n",
    "        master_action=0\n",
    "    elif iv_fluids_quantile==1 and vasopressors_quantile>1:\n",
    "        master_action=2\n",
    "    elif iv_fluids_quantile>1 and vasopressors_quantile==1:\n",
    "        master_action=1\n",
    "    else:\n",
    "        master_action = 3\n",
    "    return master_action\n",
    "\n",
    "def convert_to_unit(old_value,old_max, old_min, new_max, new_min):\n",
    "    new_value = ( (old_value - old_min) / (old_max - old_min) ) * (new_max - new_min) + new_min\n",
    "    return new_value\n",
    "\n",
    "def pre_train_master_RL(RL, data, first_run=True):\n",
    "    if first_run:\n",
    "         # reward function\n",
    "        data['reward'] = data.apply(eval('setting.' + REWARD_FUN) , axis = 1)\n",
    "        memory_array = np.concatenate([np.array(data[state_col]), \n",
    "                            np.array(data['master_action']).reshape(-1, 1),\n",
    "                            np.array(data['reward']).reshape(-1, 1), \n",
    "                            np.array(data['done']).reshape(-1, 1),\n",
    "                            np.array(data[next_state_col])],\n",
    "                            axis = 1)\n",
    "        np.save('../data/discrete/pretrain_master_memory.npy', memory_array)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        memory_array = np.load('../data/discrete/pretrain_master_memory.npy')\n",
    "\n",
    "    print('\\nSTART store_transition\\n')\n",
    "    RL.store_transition(memory_array)\n",
    "    \n",
    "    print('\\nDiscrete START PRE-TRAINING MASTER AGENT\\n')\n",
    "\n",
    "  \n",
    "    for i in tqdm(range(ITERATION_ROUND_PRETRAIN)):\n",
    "        RL.learn(i, pretrain = True)\n",
    "    loss = RL.cost_his\n",
    "    return loss\n",
    "\n",
    "def train_master_RL(RL, data, use_FM, first_run=True):\n",
    "    if first_run:\n",
    "         # reward function\n",
    "        data['reward'] = data.apply(eval('setting.' + REWARD_FUN) , axis = 1)        \n",
    "        memory_array = np.concatenate([np.array(data[state_col]),\n",
    "                                       np.array(data[next_state_col]),\n",
    "                                       np.array(data['master_action']).reshape(-1, 1),\n",
    "                                       np.array(data['reward']).reshape(-1, 1), \n",
    "                                       np.array(data['done']).reshape(-1, 1),\n",
    "                                       np.array(data['Q_phys_no_action']).reshape(-1,1),\n",
    "                                       np.array(data['Q_phys_IV_only']).reshape(-1,1),\n",
    "                                       np.array(data['Q_phys_Vasso_only']).reshape(-1,1),\n",
    "                                       np.array(data['Q_phys_qmix']).reshape(-1,1),\n",
    "                                       np.array(data[FM_context_list]),\n",
    "                                       np.array(data[next_FM_context_list])],\n",
    "                                       axis = 1)\n",
    "        np.save('../data/discrete/master_memory.npy', memory_array)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        memory_array = np.load('../data/discrete/master_memory.npy', allow_pickle=True)\n",
    "\n",
    "    print('\\nSTART store_transition\\n')\n",
    "    RL.store_transition(memory_array)\n",
    "    \n",
    "    if use_FM>0:\n",
    "        print('\\nSTART TRAINING MASTER AGENT with Embedding K{}\\n'.format(setting.hidden_factor))\n",
    "    else:\n",
    "        print('\\nSTART TRAINING MASTER AGENT WITH NO EMBEDDING\\n')\n",
    "  \n",
    "    for i in tqdm(range(ITERATION_ROUND)):\n",
    "        RL.learn(i, use_FM, pretrain = False)\n",
    "    loss = RL.cost_his\n",
    "    return loss\n",
    "\n",
    "def train_single_RL_IV(RL, data, use_FM, first_run=True, writer = None, epoch = 1):\n",
    "    if first_run:\n",
    "         # reward function\n",
    "        data['reward'] = data.apply(eval('setting.' + REWARD_FUN) , axis = 1)        \n",
    "        memory_array = np.concatenate([np.array(data[state_col]), \n",
    "                                       np.array(data[next_state_col]), \n",
    "                                       np.array(data[action_dis_col[0]]).reshape(-1,1), \n",
    "                                       np.array(data['reward']).reshape(-1, 1), \n",
    "                                       np.array(data['done']).reshape(-1, 1),\n",
    "                                       np.array(data[FM_context_list]), \n",
    "                                       np.array(data[next_FM_context_list])],\n",
    "                                       axis = 1)\n",
    "        np.save('../data/discrete/IV_only_memory.npy', memory_array)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        memory_array = np.load('../data/discrete/IV_only_memory.npy', allow_pickle=True)\n",
    "\n",
    "    print('\\nSTART store_transition\\n')\n",
    "    RL.store_transition(memory_array)\n",
    "    \n",
    "\n",
    "\n",
    "    if use_FM>0:\n",
    "        print('\\nSTART TRAINING IV AGENT with Embedding K{}\\n'.format(setting.hidden_factor))\n",
    "    else:\n",
    "        print('\\nSTART TRAINING IV AGENT WITH NO EMBEDDING\\n')\n",
    "    EPISODE = int(ITERATION_ROUND_IV)\n",
    "    for i in tqdm(range(EPISODE)):\n",
    "        RL.train_step_single_AC(i, use_FM, writer = writer, iv_action_only = True)\n",
    "        \n",
    "\n",
    "    IV_loss = RL.cost_his\n",
    "    return IV_loss\n",
    "\n",
    "def train_single_RL_Vasso(RL, data, use_FM, first_run=True,writer = None, epoch = 1):\n",
    "    if first_run:\n",
    "         # reward function\n",
    "        data['reward'] = data.apply(eval('setting.' + REWARD_FUN) , axis = 1)        \n",
    "        memory_array = np.concatenate([np.array(data[state_col]),\n",
    "                                       np.array(data[next_state_col]),\n",
    "                                       np.array(data[action_dis_col[1]]).reshape(-1,1),\n",
    "                                       np.array(data['reward']).reshape(-1, 1),\n",
    "                                       np.array(data['done']).reshape(-1, 1),\n",
    "                                       np.array(data[FM_context_list]),\n",
    "                                       np.array(data[next_FM_context_list])],\n",
    "                                       axis = 1)\n",
    "        np.save('../data/discrete/vasso_only_memory.npy', memory_array)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        memory_array = np.load('../data/discrete/vasso_only_memory.npy', allow_pickle=True)\n",
    "\n",
    "    print('\\nSTART store_transition\\n')\n",
    "    RL.store_transition(memory_array)\n",
    "    \n",
    "    if use_FM>0:\n",
    "        print('\\nSTART TRAINING VASO AGENT with Embedding K{}\\n'.format(setting.hidden_factor))\n",
    "    else:\n",
    "        print('\\nSTART TRAINING VASO AGENT WITH NO EMBEDDING\\n')\n",
    "    EPISODE = int(ITERATION_ROUND_Vaso)\n",
    "    for i in tqdm(range(EPISODE)):\n",
    "        RL.train_step_single_AC(i,use_FM, writer = writer, iv_action_only= False)\n",
    "        \n",
    "\n",
    "    Vasso_loss = RL.cost_his\n",
    "    return Vasso_loss\n",
    "\n",
    "\n",
    "def train_mixer(RL, data, use_FM, first_run=True, writer = None, epoch = 1):\n",
    "    if first_run:\n",
    "         # reward function\n",
    "        data['reward'] = data.apply(eval('setting.' + REWARD_FUN) , axis = 1)  \n",
    "        actions = data.apply(lambda x: x[action_dis_col[0]] * 5 + x[action_dis_col[1]]  -6, axis =1)\n",
    "\n",
    "        memory_array = np.concatenate([np.array(data[state_col]),\n",
    "                                       np.array(data[next_state_col]),\n",
    "                                       np.array(actions).reshape(-1, 1), \n",
    "                                       np.array(data[ai_action_dis_col]),\n",
    "                                       np.array(data['reward']).reshape(-1, 1), \n",
    "                                       np.array(data['done']).reshape(-1, 1),\n",
    "                                       np.array(data[FM_context_list]),\n",
    "                                       np.array(data[next_FM_context_list])],axis = 1)\n",
    "        np.save('../data/discrete/hierarchy_discrete_memory.npy', memory_array)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        memory_array = np.load('../data/discrete/Qmix_discrete_memory.npy')\n",
    "\n",
    "    print('\\nSTART store_transition\\n')\n",
    "    RL.store_transition(memory_array)\n",
    "    \n",
    "\n",
    "    if use_FM>0:\n",
    "        print('\\nSTART TRAINING QMIX AGENT with Embedding K{}\\n'.format(setting.hidden_factor))\n",
    "    else:\n",
    "        print('\\nSTART TRAINING QMIX AGENT WITH NO EMBEDDING\\n')\n",
    "    EPISODE = int(ITERATION_ROUND_QMIX)\n",
    "    for i in tqdm(range(EPISODE)):\n",
    "        RL.train_step(i, use_FM, writer = writer)\n",
    "        \n",
    "\n",
    "    loss = RL.cost_his\n",
    "  \n",
    "    return loss\n",
    "\n",
    "def train_function(df, use_FM, train_FM):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    seed = setting.SEED\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    tf.set_random_seed(seed)\n",
    "\n",
    "    summarize = False\n",
    "\n",
    "\n",
    "    l_state = STATE_DIM\n",
    "    l_action = setting.number_action\n",
    "    action_level = setting.action_level\n",
    "    N_agent = 2\n",
    "    master_action_num = setting.master_action_num\n",
    "    \n",
    "    data = df[df['train_test']==\"train\"]\n",
    "    data =data.reset_index(drop = True)\n",
    "    if use_FM>0:\n",
    "        input_dim = 2*setting.hidden_factor\n",
    "    else:\n",
    "        input_dim = STATE_DIM\n",
    "# ############## pre-train master agent #####################\n",
    "\n",
    "    if (train_FM>0):\n",
    "        config_proto = tf.ConfigProto()\n",
    "        config_proto.gpu_options.allow_growth = True\n",
    "        sess = tf.Session(config=config_proto)\n",
    "\n",
    "        RL_master = alg_discrete.DuelingDQN(n_actions=master_action_num, n_features=STATE_DIM, memory_size=len(data),\n",
    "                                       batch_size=BATCH_SIZE, e_greedy_increment=0.001, sess=sess, dueling=True, output_graph=True, pretrain = True, K_hidden_factor= setting.hidden_factor)\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        pre_master_loss = pre_train_master_RL(RL_master, data, first_run=True)\n",
    "        # save model\n",
    "        saver = tf.compat.v1.train.Saver()\n",
    "        saver.save(sess, 'models/pretrain/duel_DQN')\n",
    "        new_saver = tf.compat.v1.train.import_meta_graph('models/pretrain/duel_DQN.meta')\n",
    "        new_saver.restore(sess, 'models/pretrain/duel_DQN')\n",
    "    # #########################################################################    \n",
    "\n",
    "        # get embedding features from pre-train model\n",
    "        eval_batch_size = int(20000)\n",
    "        index_num_0 = int(0)\n",
    "        index_num_1 = int(eval_batch_size)\n",
    "        eval_result = pd.DataFrame(columns=list(FM_context_list) + list(next_FM_context_list)+['Q_phys_no_action'])\n",
    "        while (index_num_1 < len(df)):\n",
    "            eval_batch = df.iloc[index_num_0:index_num_1]\n",
    "            eval_q, state_embedding = sess.run([RL_master.q_eval, RL_master.Em_State],\n",
    "                                               feed_dict={RL_master.s: eval_batch[state_col]})\n",
    "\n",
    "            next_state_embedding = sess.run([RL_master.Em_State],\n",
    "                                            feed_dict={RL_master.s: eval_batch[next_state_col]})\n",
    "            context_embedding = sess.run([RL_master.Em_State],\n",
    "                                    feed_dict={RL_master.s: eval_batch[context_state_col]})\n",
    "            next_context_embedding = sess.run([RL_master.Em_State],\n",
    "                                            feed_dict={RL_master.s: eval_batch[context_next_state_col]})\n",
    "\n",
    "            state_embedding = np.array(state_embedding).reshape(-1, hidden_factor)\n",
    "            next_state_embedding = np.array(next_state_embedding).reshape(-1,hidden_factor)\n",
    "            context_embedding = np.array(context_embedding).reshape(-1,hidden_factor)\n",
    "            next_context_embedding = np.array(next_context_embedding).reshape(-1, hidden_factor)\n",
    "            Q_no = eval_q[:,0].reshape(-1,1)\n",
    "\n",
    "\n",
    "            model_result = np.concatenate([state_embedding,context_embedding, next_state_embedding, next_context_embedding,Q_no], axis=1)\n",
    "            model_result = pd.DataFrame(model_result, columns =list(eval_result.columns))\n",
    "            eval_result = eval_result.append(model_result, ignore_index=True)\n",
    "            index_num_0 = index_num_1\n",
    "            index_num_1 = index_num_1 +eval_batch_size\n",
    "        print(index_num_0)\n",
    "        eval_batch = df.iloc[index_num_0:]\n",
    "        eval_q, state_embedding = sess.run([RL_master.q_eval, RL_master.Em_State],\n",
    "                                           feed_dict={RL_master.s: eval_batch[state_col]})\n",
    "        next_state_embedding = sess.run([RL_master.Em_State],\n",
    "                                        feed_dict={RL_master.s: eval_batch[next_state_col]})\n",
    "        context_embedding = sess.run([RL_master.Em_State],\n",
    "                                feed_dict={RL_master.s: eval_batch[context_state_col]})\n",
    "        next_context_embedding = sess.run([RL_master.Em_State],\n",
    "                                        feed_dict={RL_master.s: eval_batch[context_next_state_col]}) \n",
    "        state_embedding = np.array(state_embedding).reshape(-1, hidden_factor)\n",
    "        next_state_embedding = np.array(next_state_embedding).reshape(-1,hidden_factor)\n",
    "        context_embedding = np.array(context_embedding).reshape(-1,hidden_factor)\n",
    "        next_context_embedding = np.array(next_context_embedding).reshape(-1, hidden_factor)\n",
    "        Q_no = eval_q[:,0].reshape(-1,1)    \n",
    "        model_result = np.concatenate([state_embedding,context_embedding, next_state_embedding, next_context_embedding,Q_no], axis=1)\n",
    "        model_result = pd.DataFrame(model_result, columns =list(eval_result.columns))\n",
    "        eval_result = eval_result.append(model_result, ignore_index=True)\n",
    "\n",
    "        result_array = np.concatenate([df.values, eval_result.values], axis=1)\n",
    "        result = pd.DataFrame(result_array, \n",
    "                              columns=list(df.columns)+list(eval_result.columns))\n",
    "\n",
    "\n",
    "        print(result.head(1))\n",
    "        print(len(result))\n",
    "        result.to_csv('../data/mimic_embeddings_K'+str(setting.hidden_factor)+'_Itr'+str(setting.ITERATION_ROUND_PRETRAIN)+'.csv', encoding = 'gb18030', index = False)\n",
    "        df = result.copy()\n",
    "        \n",
    "        df['Q_phys_no_action'] = df['Q_phys_no_action'].apply(lambda x: -Q_threshold if x<-Q_threshold else x if x<Q_threshold else Q_threshold)\n",
    "\n",
    "    else:\n",
    "        print(\"loading data...\")\n",
    "        df = pd.read_csv(('../data/mimic_embeddings_K'+str(setting.hidden_factor)+'_Itr'+str(setting.ITERATION_ROUND_PRETRAIN)+'.csv'), index_col = False)\n",
    "        df['master_action'] = df.apply(lambda x: compute_master_action(x['iv_fluids_quantile'], x['vasopressors_quantile']), axis=1)\n",
    "\n",
    "        df['Q_phys_no_action'] = df['Q_phys_no_action'].apply(lambda x: -Q_threshold if x<-Q_threshold else x if x<Q_threshold else Q_threshold)\n",
    "\n",
    "################### single_AC network for IV only (Discrete)################### \n",
    "\n",
    "    IV_only_data = df[(df['master_action']==1) & (df['train_test']==\"train\")]\n",
    "    IV_only_data = IV_only_data.reset_index(drop=True)\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    alg = alg_discrete.Single_AC(1, l_state, hidden_factor, input_dim, action_level, nn, 1e-3, 2e-4,memory_size=len(IV_only_data), batch_size=BATCH_SIZE, e_greedy_increment=0.001) \n",
    "    config_proto = tf.ConfigProto()\n",
    "    config_proto.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config_proto)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(alg.list_initialize_target_ops)\n",
    "    writer = tf.compat.v1.summary.FileWriter('models/IV_only', sess.graph)\n",
    "    saver = tf.compat.v1.train.Saver(max_to_keep=100) \n",
    "    \n",
    "    \n",
    "    iv_loss = train_single_RL_IV(alg, IV_only_data, use_FM, first_run=True, writer = writer,epoch = 1)\n",
    "    # save model\n",
    "    saver.save(sess, 'models/IV_only/IV_only_model.ckpt')\n",
    "\n",
    "    i = 2\n",
    "    while(i<=TOTAL_ITERATION_NUM):\n",
    "        new_saver = tf.compat.v1.train.import_meta_graph('models/IV_only/IV_only_model.ckpt.meta')\n",
    "        new_saver.restore(sess, 'models/IV_only/IV_only_model.ckpt')\n",
    "        iv_loss = train_single_RL_IV(alg, IV_only_data, first_run=False, writer = writer, epoch = i)\n",
    "        # save model\n",
    "        new_saver.save(sess, 'models/IV_only/IV_only_model.ckpt')\n",
    "        i = i+1 \n",
    "\n",
    "\n",
    "    # evaluate single IV model\n",
    "    if (use_FM>0):\n",
    "        eval_state = np.array(df[FM_context_list])\n",
    "        eval_obs = np.array(eval_state).reshape((-1,2*setting.hidden_factor))\n",
    "    else:\n",
    "        eval_state = np.array(df[state_col])\n",
    "        eval_obs = np.array(eval_state).reshape((-1,len(state_col)))\n",
    "    \n",
    "    \n",
    "    actions_int = alg.run_actor(eval_obs, sess)\n",
    "    a_0 = (df[action_dis_col[0]]-1)\n",
    "    phys_qmix = alg.run_phys_Q(sess, list_state=eval_state, list_obs = eval_obs, a_0=a_0)\n",
    "    ai_qmix = alg.run_RL_Q(sess, list_state=eval_state, list_obs = eval_obs, a_0=actions_int[:,0])\n",
    "\n",
    "    result_array = np.concatenate([df.values, actions_int, phys_qmix,ai_qmix], axis=1)\n",
    "    result = pd.DataFrame(result_array, \n",
    "                          columns=list(df.columns)+['ai_action_dis_IV_only','Q_phys_IV_only','Q_ai_IV_only'])\n",
    "\n",
    "    print(\"result\")\n",
    "    print(result.head(1))\n",
    "################### single_AC network for Vasso only (Discrete) ################### \n",
    "\n",
    "#     Vasso_only_data = result.copy()\n",
    "    df = result.copy()\n",
    "    temp = df[df['train_test']==\"train\"]\n",
    "    Vasso_only_data = temp[temp['master_action'].isin([2,3]) ]\n",
    "#     Vasso_only_data = df[(df['master_action']==2) & (df['train_test']==\"train\")]\n",
    "    Vasso_only_data = Vasso_only_data.reset_index(drop=True)\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    alg = alg_discrete.Single_AC(1, l_state, hidden_factor, input_dim, action_level, nn, 3e-4, 2e-4 ,memory_size=len(Vasso_only_data), batch_size=BATCH_SIZE, e_greedy_increment=0.001)    \n",
    "\n",
    "    config_proto = tf.ConfigProto()\n",
    "    config_proto.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config_proto)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(alg.list_initialize_target_ops)\n",
    "    writer = tf.compat.v1.summary.FileWriter('models/Vasso_only/', sess.graph)\n",
    "    saver = tf.compat.v1.train.Saver(max_to_keep=100)\n",
    "    \n",
    "    \n",
    "    vasso_loss = train_single_RL_Vasso(alg, Vasso_only_data, use_FM, first_run=True, writer = writer, epoch = 1)\n",
    "    # save model\n",
    "    saver.save(sess, 'models/Vasso_only/Vasso_only_model.ckpt')\n",
    "    i = 2\n",
    "    while(i<=TOTAL_ITERATION_NUM):\n",
    "        new_saver = tf.compat.v1.train.import_meta_graph('models/Vasso_only/Vasso_only_model.ckpt.meta')\n",
    "        new_saver.restore(sess, 'models/Vasso_only/Vasso_only_model.ckpt')\n",
    "        vasso_loss = train_single_RL_Vasso(alg, Vasso_only_data, first_run=True, writer = writer,epoch = i)\n",
    "        # save model\n",
    "        new_saver.save(sess, 'models/Vasso_only/Vasso_only.ckpt')\n",
    "        i = i+1     \n",
    "    \n",
    "    \n",
    "    # evaluate single IV model\n",
    "    if use_FM>0:\n",
    "        eval_state = np.array(df[FM_context_list])\n",
    "        eval_obs = np.array(eval_state).reshape((-1,2*setting.hidden_factor))\n",
    "\n",
    "    else:\n",
    "        eval_state = np.array(df[state_col])\n",
    "        eval_obs = np.array(eval_state).reshape((-1,len(state_col)))\n",
    "        \n",
    "\n",
    "    actions_int = alg.run_actor(eval_obs, sess)\n",
    "    a_0 = (df[action_dis_col[1]]-1)\n",
    "    phys_qmix = alg.run_phys_Q(sess, list_state=eval_state, list_obs = eval_obs, a_0=a_0)\n",
    "    ai_qmix = alg.run_RL_Q(sess, list_state=eval_state, list_obs = eval_obs, a_0=actions_int[:,0])\n",
    "    \n",
    "    result_array = np.concatenate([df.values, actions_int, phys_qmix,ai_qmix], axis=1)\n",
    "    result = pd.DataFrame(result_array, \n",
    "                          columns=list(df.columns)+['ai_action_dis_Vasso_only','Q_phys_Vasso_only','Q_ai_Vasso_only'])\n",
    "\n",
    "\n",
    "    print(\"result\")\n",
    "    print(result.head(1))\n",
    "# ################################ Qmix Discrete action ###############################\n",
    "    df = result.copy()\n",
    "    mixer_data = df[(df['master_action']==3) & (df['train_test']==\"train\")]\n",
    "    mixer_data = mixer_data.reset_index(drop = True)\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    alg = alg_discrete.Qmix_discrete(N_agent, l_state, hidden_factor, input_dim, action_level, nn, memory_size=len(mixer_data), batch_size=BATCH_SIZE, e_greedy_increment=0.001)    \n",
    "\n",
    "    config_proto = tf.ConfigProto()\n",
    "    config_proto.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config_proto)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(alg.list_initialize_target_ops)\n",
    "    writer = tf.compat.v1.summary.FileWriter('models/Qmix', sess.graph)\n",
    "    saver = tf.compat.v1.train.Saver(max_to_keep=100)\n",
    "    \n",
    "    \n",
    "    mixer_loss = train_mixer(alg, mixer_data, use_FM, first_run=True, writer = writer, epoch = 1)\n",
    "    # save model\n",
    "    saver.save(sess, 'models/qmix/model.ckpt')\n",
    "    \n",
    "    i = 2\n",
    "    while(i<=TOTAL_ITERATION_NUM):\n",
    "        new_saver = tf.compat.v1.train.import_meta_graph('models/qmix/model.ckpt.meta')\n",
    "        new_saver.restore(sess, 'models/qmix/model.ckpt')\n",
    "        loss = train_single(alg, mixer_data, first_run=False, writer = writer, epoch = i)\n",
    "        # save model\n",
    "        saver.save(sess, 'models/qmix/model.ckpt')\n",
    "        i = i+1\n",
    "    \n",
    "    # evaluate model\n",
    "    if (use_FM>0):\n",
    "        eval_state = np.array(df[FM_context_list])\n",
    "        eval_obs = np.stack((eval_state, eval_state)).reshape((-1,2*setting.hidden_factor))\n",
    "\n",
    "    else:\n",
    "        eval_state = np.array(df[state_col])\n",
    "        eval_obs = np.stack((eval_state, eval_state)).reshape((-1,len(state_col))) \n",
    "        \n",
    "\n",
    "\n",
    "    iv_only = (df[action_dis_col[0]]-1).astype('int64')\n",
    "    vasso_only = (df[action_dis_col[1]]-1).astype('int64')\n",
    "    iv_actions, vaso_actions = alg.run_actor(eval_obs, sess, iv_only = iv_only, vasso_only = vasso_only)\n",
    "    a_0 = (df[action_dis_col[0]]-1)\n",
    "    a_1 = (df[action_dis_col[1]]-1)\n",
    "\n",
    "    phys_qmix = alg.run_phys_Q(sess, list_state=eval_state, list_obs = eval_obs, a_0=a_0, a_1=a_1, iv_only = iv_only, vasso_only = vasso_only)\n",
    "    ai_qmix = alg.run_RL_Q(sess, list_state=eval_state, list_obs = eval_obs, a_0=iv_actions, a_1=vaso_actions, iv_only = iv_only, vasso_only = vasso_only)\n",
    "    \n",
    "    result_array = np.concatenate([df.values, iv_actions, vaso_actions, phys_qmix,ai_qmix], axis=1)\n",
    "    result = pd.DataFrame(result_array, \n",
    "                          columns=list(df.columns)+['ai_action_qmix_IV', 'ai_action_qmix_Vasso','Q_phys_qmix','Q_ai_qmix'])\n",
    "        \n",
    "    if use_FM>0:\n",
    "        res_dir = '../data/Disc_Qmix_result_K'+str(setting.hidden_factor)+'_Itr'+str(setting.ITERATION_ROUND_QMIX)+'.csv'\n",
    "        result.to_csv(res_dir, encoding = 'gb18030')\n",
    "    else:\n",
    "        result.to_csv('../data/Disc_Qmix_result_NoFM.csv', encoding = 'gb18030')\n",
    "################### Master Agent to decide no_action, IV_only, Vasso_only, or Qmix (Discrete)###################\n",
    "    tf.reset_default_graph()\n",
    "    df = result.copy()\n",
    "    combined_data = df[df['train_test']==\"train\"]\n",
    "    config_proto = tf.ConfigProto()\n",
    "    config_proto.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config_proto)\n",
    "\n",
    "    RL_master = alg_discrete.MasterDQN(master_action_num, STATE_DIM, hidden_factor, input_dim,\n",
    "                                   memory_size=len(combined_data),\n",
    "                                   batch_size=BATCH_SIZE, e_greedy_increment=0.001, sess=sess, \n",
    "                                   dueling=True, output_graph=True, pretrain = False)\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    master_loss = train_master_RL(RL_master, combined_data, use_FM, first_run=True)\n",
    "    # save model\n",
    "    saver = tf.compat.v1.train.Saver()\n",
    "    saver.save(sess, 'models/master/duel_DQN')\n",
    "\n",
    "#     evaluate master agent\n",
    "    if (use_FM>0):\n",
    "        eval_state = np.array(df[FM_context_list])\n",
    "        eval_obs = np.array(eval_state).reshape((-1,2*setting.hidden_factor))\n",
    "\n",
    "    else:\n",
    "        eval_state = np.array(df[state_col])\n",
    "        eval_obs = np.array(eval_state).reshape((-1,len(state_col)))     \n",
    "   \n",
    "    eval_q = sess.run(RL_master.q_eval, feed_dict={RL_master.s:eval_state})    \n",
    "    result_array = np.concatenate([df.values, eval_q], axis=1)\n",
    "    result = pd.DataFrame(result_array, \n",
    "                          columns=list(df.columns)+['Q_0', 'Q_1', 'Q_2', 'Q_3'])\n",
    "    \n",
    "\n",
    "    Q_list = ['Q_' + str(i) for i in range(master_action_num)]\n",
    "    result['Q_ai_master'] = np.max(result[Q_list],axis = 1)\n",
    "    result['ai_action_master'] = np.argmax(np.array(result[Q_list]),axis = 1)\n",
    "    print(\"result\")\n",
    "    print(result.head(1))\n",
    "    if use_FM>0:\n",
    "        res_dir = '../data/Disc_main_result_K'+str(setting.hidden_factor)+'_Itr'+str(setting.ITERATION_ROUND)+'.csv'\n",
    "        result.to_csv(res_dir, encoding = 'gb18030')\n",
    "    else:\n",
    "        result.to_csv('../data/Disc_main_result_NoFM.csv', encoding = 'gb18030')\n",
    "\n",
    " \n",
    "    run_time = display_time((time.time()-start_time))\n",
    "    print(\"done!\")   \n",
    "    print(\"Total run time with {} episodes:\\n {}\".format(setting.ITERATION_ROUND, run_time))    \n",
    "    print(\"start evaluation\")\n",
    "    train_result = result[result['train_test']==\"train\"]\n",
    "    train_result = train_result.reset_index(drop=True)\n",
    "    test_result = result[result['train_test']==\"test\"]\n",
    "    test_result = test_result.reset_index(drop =True)\n",
    "    \n",
    "    if train_FM==0:\n",
    "        pre_master_loss = 0\n",
    "    \n",
    "    if use_FM>0:\n",
    "        print(\"evaluating train result\")\n",
    "        run_eval(train_result, pre_master_loss, master_loss, iv_loss, vasso_loss, mixer_loss, datatype = 'mimic', phase = \"train_Embedding\")\n",
    "        print(\"evaluating test result\")\n",
    "        run_eval(test_result, pre_master_loss, master_loss, iv_loss, vasso_loss, mixer_loss, datatype = 'mimic', phase = \"test_Embedding\")\n",
    "    else:\n",
    "        print(\"evaluating train result\")\n",
    "        run_eval(train_result, pre_master_loss, master_loss, iv_loss, vasso_loss, mixer_loss, datatype = 'mimic', phase = \"train\")\n",
    "        print(\"evaluating test result\")\n",
    "        run_eval(test_result, pre_master_loss, master_loss, iv_loss, vasso_loss, mixer_loss, datatype = 'mimic', phase = \"test\")    \n",
    "   \n",
    "   ############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --use_FM USE_FM --retrain_FM RETRAIN_FM\n",
      "ipykernel_launcher.py: error: the following arguments are required: --use_FM/-e, --retrain_FM/-train_FM\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/home/e0998145/miniconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3513: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--use_FM', '-e', type=float, required=True)\n",
    "    parser.add_argument('--retrain_FM', '-train_FM', type=float, required=True)\n",
    "    args = parser.parse_args()  \n",
    "        \n",
    "\n",
    "  \n",
    "    df = pd.read_csv('../data/data_rl_4h_train_test_split_3steps.csv')\n",
    "    \n",
    "    df.fillna(0, inplace=True)\n",
    "    max_vaso = df.vasopressors.quantile(0.99)\n",
    "    df['vasopressors']=df['vasopressors'].apply(lambda x: x if x<max_vaso else max_vaso )\n",
    "    max_iv = df.iv_fluids.quantile(0.99)\n",
    "    df['iv_fluids']=df['iv_fluids'].apply(lambda x: x if x<max_iv else max_iv )   \n",
    "       \n",
    "    df['master_action'] = df.apply(lambda x: compute_master_action(x['iv_fluids_quantile'], x['vasopressors_quantile']), axis=1)\n",
    "    \n",
    "    print(\"process data done\")\n",
    "\n",
    "    train_function(df, args.use_FM, args.retrain_FM)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
